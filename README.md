# EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers

## Paper abstract
We study the task of finding evidence for a hypothesis in the biomedical literature. Finding relevant evidence is a necessary precursor for evaluating the validity of scientific hypotheses, and for applications such as automated meta-analyses and scientifically grounded question-answering systems. We develop a pipeline for high quality, sentence-by-sentence annotation of biomedical papers for this task. The pipeline leverages expert judgments of scientific relevance, and is validated using teams of human annotators. We evaluate a diverse set of language models and retrieval systems on the benchmark, which consists of more than 400 fully annotated papers and 80k sentence judgments. The performance of the best models still falls significantly short of human-level on this task. By providing a standardized benchmark and evaluation framework, this work will support the development of tools which automate evidence synthesis and hypothesis testing. 

